{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import crawling as aw_crawl\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noticias epidemiológicas\n",
    "\n",
    "Aproximadamente, el código demora 2h 28 minutos en recolectar las noticias de un chunk de cuatro enfermedades desde enero del 2005 hasta diciembre de 2024.\n",
    "* No hay mayores problemas con colocar fechas imposibles, salvo un mayor tiempo de procesamiento. El query en Google solo indicará que no se encontraron resultados y, dado nuestro algoritmo, ignorará estos enlaces.\n",
    "\n",
    "Si bien se ejecutó en paralelo, el tiempo total de procesamiento equivale a 17h 16 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datecombo = list(product(range(2005,2025),range(1,13)))\n",
    "with open('diseases.txt','r') as f:\n",
    "    diseases = [dis[:-1] for dis in f.readlines()]\n",
    "    \n",
    "subset = diseases[:5]\n",
    "\n",
    "for disease in subset:\n",
    "    print(f'{disease}: {dt.now()}')\n",
    "    \n",
    "    for datetuple in datecombo:\n",
    "        aw_crawl.google_crawl(disease,aw_crawl.resolve_date(datetuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: (9640, 3)\n",
      "Duplicados: 1627\n"
     ]
    }
   ],
   "source": [
    "all_news = pd.concat([pd.read_csv('urls/' + file) for file in os.listdir('urls')],axis=0,ignore_index=True)\n",
    "print(f'Dimensiones: {all_news.shape}')\n",
    "print(f'Duplicados: {all_news.duplicated(subset=['link']).sum(axis=0)}')\n",
    "# all_news.drop_duplicates(subset=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 133/9640 [01:58<54:06,  2.93it/s]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "corpus = all_news['link'].progress_apply(lambda x: pd.Series(aw_crawl.info_json(x)))\n",
    "all_news = pd.concat([all_news,corpus],axis=1)\n",
    "all_news['dups'] = all_news.duplicated(subset=['links'],keep=False)\n",
    "\n",
    "all_news.to_excel('all_news.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDC MINSA\n",
    "\n",
    "Inicialmente, tardó 10 horas (584 minutos y 46 segundos) en extraer todos los pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "años = list(range(2005,2025))\n",
    "with Pool(processes=6) as pool:\n",
    "    pool.map(aw_crawl.cdc_pdfs,años)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de PDFs recolectados: 3840\n"
     ]
    }
   ],
   "source": [
    "all_pdfs = [file for file in glob.glob('pdfs/**/*',recursive=True) if not os.path.isdir(file)]\n",
    "print(f'Cantidad de PDFs recolectados: {len(all_pdfs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
